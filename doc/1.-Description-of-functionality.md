This utility captures a snapshot of data and stores it in a offsite location along with ring information and manifest file. It is executed on one Cassandra node per execution and ideally configured to execute on all nodes in a given cluster to capture complete backup of the cluster.

Considering option of file backup (--blob-storage) is used, below is the description of the functional steps:

1. A directory with name ‘id’ provided to argument ‘--cluster’ is created in backup location specified by ‘--filebackup-location’ if not exists already. This directory will hold contents of backup snapshot for FILE backups. Cloud backups, S3 / GCP / Azure will be uploaded to buckets as specified by the --bucket parameter.
The ‘id’ used for a cluster and backup location should be same on each node so that backup directories for each 
node is created under same ‘cluster id’ directory.

2. 3 sub-directories are created under ‘cluster id’ directory in step 1 viz. Data, manifests, tokens. 

     a. The data directory holds all sub-directories for all keyspace included in backup which has  SSTables for 
     those keyspaces and corresponding tables (An SSTable is an immutable data file).

     b. The manifest directory holds a manifest file for each backup execution. 
     
     c. The tokens directory holds a yaml file with Cassandra token values from the node involved in backup 
     process. 

3. The process internally calls ‘nodetool snapshot’ utility using JMX which creates a database/keyspace/table ‘snapshot’. The contents of a ‘snapshot’ are all the SSTables required to restore a keyspace or table to the point in time that the snapshot was taken.

4. The process then copies all the SSTables which are part of snapshot created in step 3 to the backup location, only if those are not present already as a part of earlier executions. 
Note: The files which are part of current snapshot but are already present at remote location, the timestamp will be renewed to current time. This timestamp update prevents the files from being deleted by a cleanup process which considers file timestamps to delete files older than retention period. 

5. A new manifest file is created in the backup location which contains references to all the SSTables in current snapshot. So, a SSTables will be a part of more than 1 snapshots until it removed from actual Cassandra node. 
Maintaining a manifest file for each backup snapshot is leveraged by copying only SSTables which are not present in backup location for each execution of process.  

6. A new token file will be generated for each execution which contains output of ‘nodetool ring’ on the node.
